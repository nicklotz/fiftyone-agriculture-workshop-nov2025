{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FiftyOne + RoCoLe + PADIM\n",
    "\n",
    "This notebook demonstrates anomaly detection on coffee leaves using:\n",
    "1. **FiftyOne** for dataset management and visualization\n",
    "2. **ResNetR18** for feature extraction\n",
    "3. **PaDiM** (Patch Distribution Modeling) for lightweight anomaly detection\n",
    "\n",
    "The goal is to detect diseased coffee leaves by training only on healthy samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# !pip install fiftyone torch torchvision scikit-learn tqdm numpy pillow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"FiftyOne version: {fo.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Coffee Leaves Dataset\n",
    "\n",
    "Load the coffee leaves dataset from FiftyOne. This dataset contains:\n",
    "- **Normal leaves**: Healthy coffee leaves (791 samples)\n",
    "- **Anormal leaves**: Diseased coffee leaves (1538 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the coffee leaves dataset\n",
    "# If not already loaded, you may need to run the loading script first\n",
    "dataset_name = \"coffee_leaves\"\n",
    "\n",
    "if dataset_name in fo.list_datasets():\n",
    "    dataset = fo.load_dataset(dataset_name)\n",
    "    print(f\"✅ Dataset '{dataset_name}' loaded successfully\")\n",
    "else:\n",
    "    print(f\"⚠️ Dataset '{dataset_name}' not found. Loading from Hugging Face...\")\n",
    "    from fiftyone.utils.huggingface import load_from_hub\n",
    "    dataset = load_from_hub(\n",
    "        \"pjramg/Coffee_leaves_rocole_FO\",\n",
    "        name=dataset_name\n",
    "    )\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  • Name: {dataset.name}\")\n",
    "print(f\"  • Samples: {len(dataset)}\")\n",
    "print(f\"  • Media type: {dataset.media_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze labels in the dataset\n",
    "label_counts = {}\n",
    "for sample in dataset.iter_samples(progress=False):\n",
    "    label = \"unknown\"\n",
    "    if hasattr(sample, 'ground_truth') and sample.ground_truth:\n",
    "        if hasattr(sample.ground_truth, 'label'):\n",
    "            label = sample.ground_truth.label\n",
    "        elif hasattr(sample.ground_truth, 'detections') and sample.ground_truth.detections:\n",
    "            if sample.ground_truth.detections:\n",
    "                label = sample.ground_truth.detections[0].label\n",
    "    \n",
    "    label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "print(\"Label distribution:\")\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"  • {label}: {count} samples\")\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(label_counts.keys(), label_counts.values(), color=['green', 'red'])\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Coffee Leaves Dataset Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure PaDiM Model\n",
    "\n",
    "PaDiM (Patch Distribution Modeling) is a lightweight anomaly detection method that:\n",
    "- Uses pretrained CNN features (no training required)\n",
    "- Models the distribution of normal samples\n",
    "- Detects anomalies based on distance from normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BACKBONE = \"resnet18\"  # Lightweight backbone\n",
    "LAYER = \"layer3\"       # Single layer for efficiency\n",
    "IMAGE_SIZE = 224       # Standard ImageNet size\n",
    "BATCH_SIZE = 50        # Process in batches to save memory\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained backbone\n",
    "print(f\"\\nLoading {BACKBONE} backbone...\")\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                       std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Hook for feature extraction\n",
    "features = {}\n",
    "def hook_fn(module, input, output):\n",
    "    features['layer3'] = output\n",
    "\n",
    "# Register hook on layer3\n",
    "for name, module in model.named_modules():\n",
    "    if name == LAYER:\n",
    "        handle = module.register_forward_hook(hook_fn)\n",
    "        print(f\"✅ Hook registered on {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Features from Images\n",
    "\n",
    "Extract deep features from all coffee leaf images using the pretrained ResNet18.\n",
    "We process images in batches to manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_batch(filepaths):\n",
    "    \"\"\"Extract features for a batch of images\"\"\"\n",
    "    batch_features = []\n",
    "    \n",
    "    for filepath in filepaths:\n",
    "        try:\n",
    "            # Load and preprocess image\n",
    "            img = Image.open(filepath).convert('RGB')\n",
    "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                _ = model(img_tensor)\n",
    "            \n",
    "            # Get features from hook\n",
    "            feat = features['layer3']\n",
    "            \n",
    "            # Global average pooling to reduce dimensionality\n",
    "            feat = torch.nn.functional.adaptive_avg_pool2d(feat, (1, 1))\n",
    "            feat = feat.cpu().numpy().flatten()\n",
    "            batch_features.append(feat)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle missing files gracefully\n",
    "            batch_features.append(np.zeros(256))  # ResNet18 layer3 has 256 channels\n",
    "    \n",
    "    return np.array(batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all filepaths and labels\n",
    "print(\"Collecting dataset information...\")\n",
    "all_filepaths = []\n",
    "all_labels = []\n",
    "sample_ids = []\n",
    "\n",
    "for sample in dataset.iter_samples(progress=False):\n",
    "    label = \"unknown\"\n",
    "    if hasattr(sample, 'ground_truth') and sample.ground_truth:\n",
    "        if hasattr(sample.ground_truth, 'label'):\n",
    "            label = sample.ground_truth.label\n",
    "        elif hasattr(sample.ground_truth, 'detections') and sample.ground_truth.detections:\n",
    "            if sample.ground_truth.detections:\n",
    "                label = sample.ground_truth.detections[0].label\n",
    "    \n",
    "    all_filepaths.append(sample.filepath)\n",
    "    all_labels.append(label)\n",
    "    sample_ids.append(sample.id)\n",
    "\n",
    "print(f\"Collected {len(all_filepaths)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features in batches\n",
    "n_samples = len(all_filepaths)\n",
    "n_batches = (n_samples + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "all_features = []\n",
    "\n",
    "print(f\"\\nExtracting features from {n_samples} images in {n_batches} batches...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "for i in tqdm(range(0, n_samples, BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_files = all_filepaths[i:i+BATCH_SIZE]\n",
    "    batch_features = extract_features_batch(batch_files)\n",
    "    all_features.append(batch_features)\n",
    "    \n",
    "    # Clean up memory after each batch\n",
    "    gc.collect()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Concatenate all features\n",
    "all_features = np.vstack(all_features)\n",
    "print(f\"\\n✅ Extracted features shape: {all_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train PaDiM Model\n",
    "\n",
    "Train the PaDiM model by:\n",
    "1. Fitting a Gaussian distribution on healthy (normal) samples\n",
    "2. Computing anomaly scores based on distance from this distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate healthy and diseased samples\n",
    "healthy_mask = np.array([(str(l).lower() == 'normal') for l in all_labels])\n",
    "healthy_features = all_features[healthy_mask]\n",
    "anomaly_features = all_features[~healthy_mask]\n",
    "\n",
    "print(f\"Training set (healthy): {len(healthy_features)} samples\")\n",
    "print(f\"Test set (anomalies): {len(anomaly_features)} samples\")\n",
    "\n",
    "# Visualize feature distribution using first 2 principal components\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "features_2d = pca.fit_transform(all_features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(features_2d[healthy_mask, 0], features_2d[healthy_mask, 1], \n",
    "           c='green', alpha=0.5, label='Normal', s=10)\n",
    "plt.scatter(features_2d[~healthy_mask, 0], features_2d[~healthy_mask, 1], \n",
    "           c='red', alpha=0.5, label='Anormal', s=10)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Feature Distribution (PCA Visualization)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(healthy_features) > 10:  # Need minimum samples for training\n",
    "    print(\"Training PaDiM model...\")\n",
    "    \n",
    "    # Calculate mean and covariance of healthy distribution\n",
    "    mean = np.mean(healthy_features, axis=0)\n",
    "    \n",
    "    # Calculate covariance with regularization for numerical stability\n",
    "    try:\n",
    "        cov = np.cov(healthy_features.T)\n",
    "        # Add small value to diagonal for numerical stability\n",
    "        cov += np.eye(cov.shape[0]) * 1e-5\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        print(\"✅ Covariance matrix computed successfully\")\n",
    "        use_mahalanobis = True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Using simplified Euclidean distance due to: {e}\")\n",
    "        inv_cov = np.eye(mean.shape[0])\n",
    "        use_mahalanobis = False\n",
    "    \n",
    "    print(f\"Model parameters:\")\n",
    "    print(f\"  • Feature dimension: {mean.shape[0]}\")\n",
    "    print(f\"  • Distance metric: {'Mahalanobis' if use_mahalanobis else 'Euclidean'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ Not enough healthy samples for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly scores for all samples\n",
    "print(\"Calculating anomaly scores...\")\n",
    "anomaly_scores = []\n",
    "\n",
    "for feat in tqdm(all_features, desc=\"Computing scores\"):\n",
    "    # Calculate distance from normal distribution\n",
    "    diff = feat - mean\n",
    "    \n",
    "    if use_mahalanobis:\n",
    "        # Mahalanobis distance\n",
    "        score = np.sqrt(np.dot(np.dot(diff, inv_cov), diff.T))\n",
    "    else:\n",
    "        # Euclidean distance (more stable)\n",
    "        score = np.sqrt(np.dot(diff, diff))\n",
    "    \n",
    "    anomaly_scores.append(float(score))\n",
    "\n",
    "# Convert to numpy array\n",
    "anomaly_scores = np.array(anomaly_scores)\n",
    "\n",
    "# Normalize scores to [0, 1]\n",
    "if anomaly_scores.max() > anomaly_scores.min():\n",
    "    anomaly_scores = (anomaly_scores - anomaly_scores.min()) / (anomaly_scores.max() - anomaly_scores.min())\n",
    "\n",
    "print(f\"\\n✅ Computed anomaly scores for {len(anomaly_scores)} samples\")\n",
    "print(f\"Score statistics:\")\n",
    "print(f\"  • Min: {anomaly_scores.min():.4f}\")\n",
    "print(f\"  • Max: {anomaly_scores.max():.4f}\")\n",
    "print(f\"  • Mean: {anomaly_scores.mean():.4f}\")\n",
    "print(f\"  • Std: {anomaly_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize anomaly score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of scores by class\n",
    "axes[0].hist(anomaly_scores[healthy_mask], bins=30, alpha=0.6, \n",
    "            color='green', label='Normal', density=True)\n",
    "axes[0].hist(anomaly_scores[~healthy_mask], bins=30, alpha=0.6, \n",
    "            color='red', label='Anormal', density=True)\n",
    "axes[0].set_xlabel('Anomaly Score')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Anomaly Score Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot comparison\n",
    "data_to_plot = [anomaly_scores[healthy_mask], anomaly_scores[~healthy_mask]]\n",
    "box = axes[1].boxplot(data_to_plot, labels=['Normal', 'Anormal'], \n",
    "                      patch_artist=True)\n",
    "box['boxes'][0].set_facecolor('green')\n",
    "box['boxes'][1].set_facecolor('red')\n",
    "axes[1].set_ylabel('Anomaly Score')\n",
    "axes[1].set_title('Anomaly Score Comparison')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate separation metrics\n",
    "normal_mean = anomaly_scores[healthy_mask].mean()\n",
    "anormal_mean = anomaly_scores[~healthy_mask].mean()\n",
    "print(f\"\\nClass separation:\")\n",
    "print(f\"  • Normal mean score: {normal_mean:.4f}\")\n",
    "print(f\"  • Anormal mean score: {anormal_mean:.4f}\")\n",
    "print(f\"  • Separation: {anormal_mean - normal_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Embeddings and Visualization\n",
    "\n",
    "Create 2D embeddings for visualization in FiftyOne using multiple techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: PCA-based embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Computing PCA embeddings...\")\n",
    "pca = PCA(n_components=2)\n",
    "pca_embeddings = pca.fit_transform(all_features)\n",
    "print(f\"✅ PCA embeddings shape: {pca_embeddings.shape}\")\n",
    "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: UMAP-based embeddings (if available)\n",
    "try:\n",
    "    import umap\n",
    "    print(\"Computing UMAP embeddings...\")\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15)\n",
    "    umap_embeddings = reducer.fit_transform(all_features)\n",
    "    print(f\"✅ UMAP embeddings shape: {umap_embeddings.shape}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ UMAP not installed. Install with: pip install umap-learn\")\n",
    "    umap_embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Anomaly score-based circular embeddings\n",
    "print(\"Computing anomaly-based embeddings...\")\n",
    "# Map anomaly scores to a circle for intuitive visualization\n",
    "# Normal samples (low scores) near center, anomalies (high scores) on periphery\n",
    "angles = anomaly_scores * 2 * np.pi\n",
    "radii = anomaly_scores\n",
    "anomaly_embeddings = np.column_stack([\n",
    "    radii * np.cos(angles),\n",
    "    radii * np.sin(angles)\n",
    "])\n",
    "print(f\"✅ Anomaly embeddings shape: {anomaly_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all embeddings\n",
    "n_methods = 3 if umap_embeddings is not None else 2\n",
    "fig, axes = plt.subplots(1, n_methods, figsize=(6*n_methods, 5))\n",
    "\n",
    "# PCA visualization\n",
    "axes[0].scatter(pca_embeddings[healthy_mask, 0], pca_embeddings[healthy_mask, 1], \n",
    "               c='green', alpha=0.5, label='Normal', s=10)\n",
    "axes[0].scatter(pca_embeddings[~healthy_mask, 0], pca_embeddings[~healthy_mask, 1], \n",
    "               c='red', alpha=0.5, label='Anormal', s=10)\n",
    "axes[0].set_title('PCA Embeddings')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Anomaly score visualization\n",
    "axes[1].scatter(anomaly_embeddings[healthy_mask, 0], anomaly_embeddings[healthy_mask, 1], \n",
    "               c='green', alpha=0.5, label='Normal', s=10)\n",
    "axes[1].scatter(anomaly_embeddings[~healthy_mask, 0], anomaly_embeddings[~healthy_mask, 1], \n",
    "               c='red', alpha=0.5, label='Anormal', s=10)\n",
    "axes[1].set_title('Anomaly Score Embeddings (Circular)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# UMAP visualization (if available)\n",
    "if umap_embeddings is not None:\n",
    "    axes[2].scatter(umap_embeddings[healthy_mask, 0], umap_embeddings[healthy_mask, 1], \n",
    "                   c='green', alpha=0.5, label='Normal', s=10)\n",
    "    axes[2].scatter(umap_embeddings[~healthy_mask, 0], umap_embeddings[~healthy_mask, 1], \n",
    "                   c='red', alpha=0.5, label='Anormal', s=10)\n",
    "    axes[2].set_title('UMAP Embeddings')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add Results to FiftyOne Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly scores to dataset\n",
    "print(\"Adding PaDiM results to FiftyOne dataset...\")\n",
    "\n",
    "for sample_id, score in tqdm(zip(sample_ids, anomaly_scores), \n",
    "                             total=len(sample_ids), \n",
    "                             desc=\"Updating samples\"):\n",
    "    sample = dataset[sample_id]\n",
    "    sample[\"padim_score\"] = float(score)\n",
    "    sample.save()\n",
    "\n",
    "print(\"✅ Anomaly scores added to dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PCA visualization to FiftyOne\n",
    "print(\"\\nAdding PCA visualization to FiftyOne...\")\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=pca_embeddings,\n",
    "    brain_key=\"padim_pca\",\n",
    "    method=\"manual\"\n",
    ")\n",
    "print(\"✅ Added 'padim_pca' visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add anomaly score visualization to FiftyOne\n",
    "print(\"\\nAdding anomaly score visualization to FiftyOne...\")\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=anomaly_embeddings,\n",
    "    brain_key=\"padim_anomaly\",\n",
    "    method=\"manual\"\n",
    ")\n",
    "print(\"✅ Added 'padim_anomaly' visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UMAP visualization if computed\n",
    "if umap_embeddings is not None:\n",
    "    print(\"\\nAdding UMAP visualization to FiftyOne...\")\n",
    "    fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=umap_embeddings,\n",
    "        brain_key=\"padim_umap\",\n",
    "        method=\"manual\"\n",
    "    )\n",
    "    print(\"✅ Added 'padim_umap' visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Launch FiftyOne App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch FiftyOne app to explore results\n",
    "print(\"Launching FiftyOne app...\")\n",
    "print(\"\\nYou can now:\")\n",
    "print(\"  • Sort by 'padim_score' to find most anomalous leaves\")\n",
    "print(\"  • Use the embeddings panel to explore clusters\")\n",
    "print(\"  • Filter samples by score threshold\")\n",
    "print(\"  • Compare normal vs anormal samples visually\")\n",
    "\n",
    "session = fo.launch_app(dataset)\n",
    "\n",
    "# Create a view of the most anomalous samples\n",
    "anomalous_view = dataset.sort_by(\"padim_score\", reverse=True).limit(100)\n",
    "session.view = anomalous_view\n",
    "\n",
    "print(\"\\n✨ Top 100 most anomalous samples displayed in app\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
