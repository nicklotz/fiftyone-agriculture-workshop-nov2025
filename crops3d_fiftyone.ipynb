{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FiftyOne + Crops3D\n",
        "\n",
        "This notebook demonstrates how to:\n",
        "1. Install and set up FiftyOne\n",
        "2. Load the crops3d dataset from Hugging Face\n",
        "3. Compute geometric embeddings from 3D point clouds\n",
        "4. Visualize embeddings and perform similarity search\n",
        "\n",
        "**Dataset**: crops3d - 1,180 3D point cloud scans of agricultural crops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation and Setup\n",
        "\n",
        "Install FiftyOne and required libraries for 3D point cloud processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install FiftyOne and dependencies\n",
        "!pip install fiftyone\n",
        "\n",
        "# Install Hugging Face Hub for dataset download\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Install 3D processing libraries\n",
        "!pip install open3d trimesh\n",
        "\n",
        "# Install visualization dependencies\n",
        "!pip install umap-learn scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download and Load the crops3d Dataset\n",
        "\n",
        "The crops3d dataset contains PLY files (point clouds) of various agricultural crops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from huggingface_hub import snapshot_download, login\n",
        "import fiftyone as fo\n",
        "\n",
        "# Optional: Set your Hugging Face token to avoid rate limits\n",
        "HF_TOKEN = None  # Replace with your token or set as environment variable\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "\n",
        "print(\"Downloading crops3d dataset from Hugging Face...\")\n",
        "print(\"This may take a while as the dataset contains ~2.3GB of 3D data...\\n\")\n",
        "\n",
        "# Download the dataset\n",
        "snapshot_download(\n",
        "    repo_id=\"Voxel51/crops3d\",\n",
        "    local_dir=\"./crops3d_data\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n",
        "\n",
        "print(\"Download completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset into FiftyOne\n",
        "dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=\"./crops3d_data\",\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    name=\"crops3d\"\n",
        ")\n",
        "\n",
        "print(f\"Dataset loaded: {dataset.name}\")\n",
        "print(f\"Number of samples: {len(dataset)}\")\n",
        "print(f\"Media type: {dataset.media_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix PLY paths to be absolute (required for FiftyOne 3D visualization)\n",
        "def update_dataset_ply_paths(dataset):\n",
        "    \"\"\"Update PLY file paths in FiftyOne 3D dataset to use absolute paths.\"\"\"\n",
        "    for sample in dataset:\n",
        "        fo3d_filepath = sample.filepath\n",
        "        fo3d_directory = os.path.dirname(fo3d_filepath)\n",
        "        \n",
        "        with open(fo3d_filepath, 'r') as f:\n",
        "            fo3d_data = json.load(f)\n",
        "        \n",
        "        for child in fo3d_data.get('children', []):\n",
        "            if child.get('_type') == 'PlyMesh' and 'plyPath' in child:\n",
        "                child['plyPath'] = os.path.join(fo3d_directory, child['plyPath'])\n",
        "        \n",
        "        with open(fo3d_filepath, 'w') as f:\n",
        "            json.dump(fo3d_data, f, indent=2)\n",
        "\n",
        "print(\"Updating PLY paths...\")\n",
        "update_dataset_ply_paths(dataset)\n",
        "print(\"Paths updated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Computing Embeddings from 3D Point Clouds\n",
        "\n",
        "We'll extract geometric and color features from the PLY files to create meaningful embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WARNING! This might take a while\n",
        "\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def compute_point_cloud_features(ply_path):\n",
        "    \"\"\"\n",
        "    Features extracted:\n",
        "    - Spatial: height, width, volume, density (17 features)\n",
        "    - Color: RGB statistics, vegetation indices (11 features)\n",
        "    - Shape: surface normals, roughness (2 features)\n",
        "    \n",
        "    Returns:\n",
        "        numpy array of 30 features\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load point cloud\n",
        "        pcd = o3d.io.read_point_cloud(str(ply_path))\n",
        "        points = np.asarray(pcd.points)\n",
        "        colors = np.asarray(pcd.colors) if pcd.has_colors() else None\n",
        "        \n",
        "        features = []\n",
        "        \n",
        "        # === SPATIAL FEATURES (important for crop structure) ===\n",
        "        if len(points) > 0:\n",
        "            # Height features (Z-axis)\n",
        "            z_coords = points[:, 2]\n",
        "            features.extend([\n",
        "                np.max(z_coords),           # Max height (plant height)\n",
        "                np.min(z_coords),           # Min height\n",
        "                np.mean(z_coords),          # Mean height\n",
        "                np.std(z_coords),           # Height variation\n",
        "                np.percentile(z_coords, 90) \n",
        "            ])\n",
        "            \n",
        "            # Horizontal spread (canopy width)\n",
        "            x_coords = points[:, 0]\n",
        "            y_coords = points[:, 1]\n",
        "            features.extend([\n",
        "                np.ptp(x_coords),  # X range (width)\n",
        "                np.ptp(y_coords),  # Y range (depth)\n",
        "                np.std(x_coords),  # X spread\n",
        "                np.std(y_coords)   # Y spread\n",
        "            ])\n",
        "            \n",
        "            # Centroid\n",
        "            centroid = np.mean(points, axis=0)\n",
        "            features.extend(centroid) \n",
        "            \n",
        "            # Volume estimation \n",
        "            if len(points) > 10:\n",
        "                hull, _ = pcd.compute_convex_hull()\n",
        "                hull_volume = hull.get_volume()\n",
        "                features.append(np.log1p(hull_volume))  # Log scale\n",
        "            else:\n",
        "                features.append(0)\n",
        "            \n",
        "            # Point density\n",
        "            features.append(len(points))  # Total points\n",
        "            bbox_volume = np.ptp(x_coords) * np.ptp(y_coords) * np.ptp(z_coords)\n",
        "            if bbox_volume > 0:\n",
        "                features.append(len(points) / bbox_volume)  # Density\n",
        "            else:\n",
        "                features.append(0)\n",
        "            \n",
        "            # Distance from center statistics\n",
        "            distances = np.linalg.norm(points - centroid, axis=1)\n",
        "            features.extend([\n",
        "                np.mean(distances),  # Mean radius\n",
        "                np.std(distances)    # Radius variation\n",
        "            ])\n",
        "        else:\n",
        "            features.extend([0] * 17)\n",
        "        \n",
        "        # === COLOR FEATURES (important for plant health) ===\n",
        "        if colors is not None and len(colors) > 0:\n",
        "            # RGB statistics\n",
        "            features.extend([\n",
        "                np.mean(colors[:, 0]),  # Mean red\n",
        "                np.mean(colors[:, 1]),  # Mean green\n",
        "                np.mean(colors[:, 2]),  # Mean blue\n",
        "                np.std(colors[:, 0]),   # Red variation\n",
        "                np.std(colors[:, 1]),   # Green variation\n",
        "                np.std(colors[:, 2])    # Blue variation\n",
        "            ])\n",
        "            \n",
        "            # Vegetation indices\n",
        "            r, g, b = colors[:, 0], colors[:, 1], colors[:, 2]\n",
        "            \n",
        "            # Greenness index (higher = healthier vegetation)\n",
        "            greenness = g - 0.5 * (r + b)\n",
        "            features.extend([\n",
        "                np.mean(greenness),\n",
        "                np.std(greenness)\n",
        "            ])\n",
        "            \n",
        "            # Normalized Green-Red Difference Index\n",
        "            with np.errstate(divide='ignore', invalid='ignore'):\n",
        "                ngrd = (g - r) / (g + r + 1e-10)\n",
        "                ngrd = np.nan_to_num(ngrd)\n",
        "            features.append(np.mean(ngrd))\n",
        "            \n",
        "            # Color entropy (diversity)\n",
        "            hist, _ = np.histogramdd(colors, bins=8)\n",
        "            hist = hist.flatten() + 1e-10\n",
        "            hist = hist / hist.sum()\n",
        "            entropy = -np.sum(hist * np.log(hist))\n",
        "            features.append(entropy)\n",
        "            \n",
        "            # Dominant color channel\n",
        "            mean_colors = [np.mean(r), np.mean(g), np.mean(b)]\n",
        "            features.append(np.argmax(mean_colors))  # 0=red, 1=green, 2=blue\n",
        "        else:\n",
        "            features.extend([0] * 11)\n",
        "        \n",
        "        # === SHAPE FEATURES ===\n",
        "        if len(points) > 100:\n",
        "            # Estimate normals\n",
        "            pcd.estimate_normals()\n",
        "            normals = np.asarray(pcd.normals)\n",
        "            \n",
        "            if len(normals) > 0:\n",
        "                # Normal variation (surface roughness)\n",
        "                features.append(np.std(normals[:, 2])) \n",
        "                \n",
        "                # Planarity (how flat is the top)\n",
        "                z_normals = np.abs(normals[:, 2])\n",
        "                features.append(np.mean(z_normals))\n",
        "            else:\n",
        "                features.extend([0] * 2)\n",
        "        else:\n",
        "            features.extend([0] * 2)\n",
        "            \n",
        "        return np.array(features, dtype=np.float32)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {ply_path}: {e}\")\n",
        "        return np.zeros(30, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute embeddings for all samples\n",
        "print(\"Computing geometric features for each point cloud...\")\n",
        "print(\"This may take a while for 1,180 samples...\\n\")\n",
        "\n",
        "embeddings = []\n",
        "sample_ids = []\n",
        "\n",
        "# Process samples\n",
        "for sample in tqdm(dataset.iter_samples(), total=len(dataset), desc=\"Processing\"):\n",
        "    # Get PLY file path\n",
        "    ply_path = Path(sample.filepath).with_suffix('.ply')\n",
        "    if not ply_path.exists():\n",
        "        ply_path = Path(sample.filepath.replace('.fo3d', '.ply'))\n",
        "    \n",
        "    # Compute features\n",
        "    features = compute_point_cloud_features(ply_path)\n",
        "    embeddings.append(features)\n",
        "    sample_ids.append(sample.id)\n",
        "\n",
        "embeddings = np.array(embeddings)\n",
        "print(f\"\\nComputed {embeddings.shape[0]} embeddings of dimension {embeddings.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Add Embeddings to FiftyOne for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize embeddings\n",
        "scaler = StandardScaler()\n",
        "embeddings_normalized = scaler.fit_transform(embeddings)\n",
        "\n",
        "# Add UMAP visualization\n",
        "print(\"Computing UMAP visualization...\")\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=embeddings_normalized,\n",
        "    brain_key=\"geometric_embeddings\",\n",
        "    method=\"umap\",\n",
        "    num_dims=2,\n",
        "    verbose=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Add similarity index\n",
        "print(\"Building similarity index...\")\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    embeddings=embeddings_normalized,\n",
        "    brain_key=\"geometric_similarity\",\n",
        "    metric=\"euclidean\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Embeddings added to dataset!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Launch FiftyOne App for Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the FiftyOne App\n",
        "session = fo.launch_app(dataset, port=5151)\n",
        "\n",
        "print(\"ðŸš€ FiftyOne App launched!\")\n",
        "print(\"\\nðŸ“Š Available in the Embeddings panel:\")\n",
        "print(\"  â€¢ geometric_embeddings - UMAP visualization\")\n",
        "print(\"  â€¢ geometric_similarity - For finding similar crops\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
